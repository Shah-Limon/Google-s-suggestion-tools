name: US Google Search Data Extractor

on:
  workflow_dispatch:  # Allows manual triggering
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sundays at midnight

jobs:
  extract-search-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 webdriver-manager pandas requests fake-useragent tqdm
          
      - name: Setup Chrome and GeoIP
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip xvfb libxi6
          
          # Modern Chrome installation compatible with newer Ubuntu
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt-get -f install -y
          
          # Set locale to US
          sudo apt-get install -y locales
          sudo locale-gen en_US.UTF-8
          
          # Use a US proxy if needed (optional, using environment variables)
          echo "Setting up for US-based searches"

      - name: Run extraction script
        run: python scripts/keyword_extractor.py
        env:
          COUNTRY: "us"
        
      - name: Clean up data
        run: |
          python - <<EOF
          import json
          import os
          from pathlib import Path
          
          data_dir = Path("data")
          
          # Process all JSON files in the data directory
          for file_path in data_dir.glob("*.json"):
              if file_path.name == "summary_report.json":
                  continue
                  
              try:
                  with open(file_path, "r") as f:
                      data = json.load(f)
                      
                  # Clean up "people_also_search_for" data
                  if "people_also_search_for" in data:
                      # Remove entries with common unwanted phrases
                      unwanted = ["more products", "see more", "view all", "shop now", "curbside", "pick up today"]
                      data["people_also_search_for"] = [
                          item for item in data["people_also_search_for"] 
                          if item and not any(phrase in item.lower() for phrase in unwanted)
                      ]
                      
                      # Remove entries that are just numbers or very short
                      data["people_also_search_for"] = [
                          item for item in data["people_also_search_for"] 
                          if len(item) > 3 and not item.replace(".", "").isdigit()
                      ]
                      
                  # Write the cleaned data back
                  with open(file_path, "w") as f:
                      json.dump(data, f, indent=2)
                      
              except Exception as e:
                  print(f"Error processing {file_path}: {e}")
          EOF
        
      - name: Commit and push results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/
          git commit -m "Update US keyword data $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push