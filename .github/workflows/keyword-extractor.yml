name: US Google Search Data Extractor

on:
  workflow_dispatch:  # Allows manual triggering
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sundays at midnight

jobs:
  extract-search-data:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 webdriver-manager pandas requests fake-useragent tqdm
          
      - name: Setup Chrome and GeoIP
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip xvfb libxi6
          
          # Modern Chrome installation compatible with newer Ubuntu
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt-get -f install -y
          
          # Set locale to US
          sudo apt-get install -y locales
          sudo locale-gen en_US.UTF-8
          
          # Use a US proxy if needed (optional, using environment variables)
          echo "Setting up for US-based searches"

      - name: Run extraction script
        run: python scripts/keyword_extractor.py
        env:
          COUNTRY: "us"
          HEADLESS: "true"
          WAIT_TIME: "10"
        
      - name: Run data cleanup
        run: python scripts/data_cleanup.py
        
      - name: Commit and push results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/
          git commit -m "Update US keyword data $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push